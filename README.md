# üöÄ Airflow + Docker Demo Project

Este proyecto demuestra c√≥mo orquestar flujos de trabajo en **Apache Airflow** usando **Docker** y **Visual Studio Code**.  
Incluye ejemplos pr√°cticos de distintos tipos de operadores, sensores, reglas de ejecuci√≥n, paso de datos entre tareas y branching, todo con c√≥digo limpio, comentado y f√°cilmente replicable.

---

## üìÇ Estructura del proyecto

```
airflow-docker-vscode/
‚îú‚îÄ .env                   # Variables de entorno (UID/GID)
‚îú‚îÄ docker-compose.yml     # Orquestaci√≥n de servicios (Airflow, Postgres)
‚îú‚îÄ Dockerfile             # Imagen personalizada con dependencias extra
‚îú‚îÄ requirements.txt       # Dependencias Python adicionales
‚îú‚îÄ logs/                  # Logs de Airflow
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ incoming/           # Archivos externos (para FileSensor)
‚îÇ  ‚îî‚îÄ tmp/                # Archivos temporales generados en DAGs
‚îú‚îÄ dags/
‚îÇ  ‚îú‚îÄ dag_00_basic_pipeline.py
‚îÇ  ‚îú‚îÄ dag_01_file_sensors.py
‚îÇ  ‚îú‚îÄ dag_02_producer.py
‚îÇ  ‚îú‚îÄ dag_03_consumer_external_sensor.py
‚îÇ  ‚îî‚îÄ dag_04_branching.py
‚îî‚îÄ plugins/
   ‚îî‚îÄ operators/
      ‚îî‚îÄ word_count_operator.py
```

---

## ‚öôÔ∏è Requisitos previos

- **Docker** y **Docker Compose**
- **Visual Studio Code** con extensiones recomendadas:
  - Docker
  - Python
  - Dev Containers (opcional)

---

## ‚ñ∂Ô∏è Ejecuci√≥n desde VS Code

1. **Abrir proyecto en VS Code**  
   `File > Open Folder...` ‚Üí seleccionar `airflow-docker-vscode`.

2. **Configurar archivo `.env`**  
   - Linux/Mac:  
     ```bash
     echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" > .env
     ```  
   - Windows (manual):  
     ```
     AIRFLOW_UID=50000
     AIRFLOW_GID=0
     ```

3. **Inicializar base de datos y crear usuario admin**  
   ```bash
   docker compose up airflow-init
   ```

4. **Levantar servicios principales (scheduler, webserver, triggerer, postgres)**  
   ```bash
   docker compose up -d
   ```

5. **Acceder a la interfaz de Airflow**  
   - URL: [http://localhost:8080](http://localhost:8080)  
   - Usuario: `admin`  
   - Contrase√±a: `admin`

6. **Ejecutar DAGs desde la UI**  
   Activa y corre los DAGs de ejemplo (`basic_pipeline`, `file_sensors`, etc.) para observar el comportamiento.

7. **Detener servicios cuando termines**  
   ```bash
   docker compose down
   ```

---

## üõ†Ô∏è ¬øQu√© se implement√≥?

El proyecto incluye varios casos pr√°cticos, cada uno ilustrando un concepto clave de Airflow.

### 1Ô∏è‚É£ BashOperator y PythonOperator

Archivo: `dags/dag_00_basic_pipeline.py`

```python
hello_bash = BashOperator(
    task_id="hello_bash",
    bash_command='echo "Hola desde BashOperator üëã"',
)

def extract_data(**context):
    return {"records": 100, "path": "/opt/airflow/data/tmp/data.csv"}

extract = PythonOperator(
    task_id="extract",
    python_callable=extract_data,
)
```

üìå **Sugerencia de ejecuci√≥n:**  
Corre el DAG `basic_pipeline` y observa c√≥mo `extract` devuelve un diccionario que luego se usa en transformaciones.

---

### 2Ô∏è‚É£ XComs (Cross-Communications)

En el mismo DAG:

```python
push_from_bash = BashOperator(
    task_id="push_from_bash",
    bash_command='echo "42"',
    do_xcom_push=True,  # Env√≠a el valor a XCom
)

def transform_data(**context):
    ti = context["ti"]
    extracted = ti.xcom_pull(task_ids="extract")
    bash_value = int(ti.xcom_pull(task_ids="push_from_bash"))
    return {"total": extracted["records"] + bash_value}
```

üìå **Sugerencia de ejecuci√≥n:**  
Desde la UI de Airflow, abre la vista de XComs y revisa c√≥mo se pasan datos entre tareas.

---

### 3Ô∏è‚É£ Custom Operator

Archivo: `plugins/operators/word_count_operator.py`

```python
class WordCountOperator(BaseOperator):
    template_fields = ("filepath",)

    def execute(self, context):
        with open(self.filepath, "r", encoding="utf-8") as f:
            text = f.read()
        return len(text.split())
```

üìå **Sugerencia de ejecuci√≥n:**  
Ejecuta la tarea `count_words` en el DAG `basic_pipeline` y observa c√≥mo retorna el conteo de palabras de un archivo generado previamente.

---

### 4Ô∏è‚É£ Trigger Rules

En `basic_pipeline` se configuran distintos escenarios:

```python
join_ok = EmptyOperator(
    task_id="join_ok",
    trigger_rule="none_failed_min_one_success",
)

cleanup_on_failure = BashOperator(
    task_id="cleanup_on_failure",
    bash_command='echo "Limpiando tras fallo"',
    trigger_rule="one_failed",
)

always_finalize = BashOperator(
    task_id="always_finalize",
    bash_command='echo "Pipeline finalizado"',
    trigger_rule="all_done",
)
```

üìå **Sugerencia de ejecuci√≥n:**  
Forza fallos en algunas tareas y revisa qu√© rutas se ejecutan seg√∫n la regla de disparo.

---

### 5Ô∏è‚É£ FileSensor

Archivo: `dags/dag_01_file_sensors.py`

```python
wait_external_flag = FileSensor(
    task_id="wait_external_flag",
    fs_conn_id="fs_default",
    filepath="incoming/external_ready.flag",
    poke_interval=10,
    timeout=600,
    mode="reschedule",
)
```

üìå **Sugerencia de ejecuci√≥n:**  
Crea un archivo desde tu host:  
```bash
echo listo > data/incoming/external_ready.flag
```

El DAG continuar√° al detectarlo.

---

### 6Ô∏è‚É£ ExternalTaskSensor

`consumer_dag` espera a `producer_dag`:

```python
wait_for_producer = ExternalTaskSensor(
    task_id="wait_for_producer",
    external_dag_id="producer_dag",
    external_task_id="producer_complete",
    allowed_states=["success"],
)
```

üìå **Sugerencia de ejecuci√≥n:**  
Ejecuta primero `producer_dag` y luego activa `consumer_dag`. Ver√°s c√≥mo espera la finalizaci√≥n antes de continuar.

---

### 7Ô∏è‚É£ BranchPythonOperator

Archivo: `dags/dag_04_branching.py`

```python
def decide_branch(**context):
    value = context["ti"].xcom_pull(task_ids="generate_metric")
    return "process_small" if value < 50 else "process_large"

branch = BranchPythonOperator(
    task_id="branch_logic",
    python_callable=decide_branch,
)
```

üìå **Sugerencia de ejecuci√≥n:**  
Cada ejecuci√≥n elegir√° una rama diferente (seg√∫n un valor aleatorio). El flujo se unifica con un `merge` que usa `trigger_rule`.

---

## ‚úÖ Conclusi√≥n

Este proyecto te gu√≠a en la pr√°ctica de conceptos clave de Airflow:

- Operadores b√°sicos y personalizados
- Comunicaci√≥n entre tareas (XComs)
- Reglas de disparo (trigger rules)
- Sensores para archivos y dependencias externas
- Ramas condicionales en DAGs

Es un punto de partida s√≥lido para proyectos reales de orquestaci√≥n de datos.
